{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO,A2C,DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import traci\n",
    "import sumolib\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "import warnings \n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', \n",
    "                        message=\"You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments.\",\n",
    "                        category=UserWarning, \n",
    "                        module='stable_baselines3.common.vec_env.patch_gym')\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', \n",
    "                        message=\"Environment variable SUMO_HOME is not set properly, disabling XML validation.\",\n",
    "                        category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    filename='traffic_signal.log',\n",
    "                    filemode='w')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "configfile_path = \"working/fukk_1.sumocfg\"\n",
    "routefile_path = \"working/route_gen_3.rou.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_routefile():\n",
    "    # random.seed(1)\n",
    "    N = 3600\n",
    "\n",
    "    pWE= 1. / 10\n",
    "    pEW = 1. / 11\n",
    "    pNS = 1. / 10\n",
    "    pSN = 1. / 11\n",
    "\n",
    "    with open(routefile_path,\"w\") as routes:\n",
    "        print(\"\"\"<routes>\n",
    "        <vType id=\"car\" vClass=\"passenger\" length=\"4\" maxSpeed=\"25.0\" accel=\"2.6\" decel=\"4.5\"/>\n",
    "        <vType id=\"truck\" vClass=\"truck\" length=\"10\" maxSpeed=\"20.0\" accel=\"1.2\" decel=\"2.5\"/>\n",
    "\n",
    "        <route id=\"r_0\" edges=\"3i 1o\"/>\n",
    "        <route id=\"r_1\" edges=\"3i 4o\"/>\n",
    "        <route id=\"r_10\" edges=\"2i 3o\"/>\n",
    "        <route id=\"r_11\" edges=\"2i 4o\"/>\n",
    "        <route id=\"r_2\" edges=\"3i 2o\"/>\n",
    "        <route id=\"r_3\" edges=\"4i 3o\"/>\n",
    "        <route id=\"r_4\" edges=\"4i 2o\"/>\n",
    "        <route id=\"r_5\" edges=\"4i 1o\"/>\n",
    "        <route id=\"r_6\" edges=\"1i 2o\"/>\n",
    "        <route id=\"r_7\" edges=\"1i 4o\"/>\n",
    "        <route id=\"r_8\" edges=\"1i 3o\"/>\n",
    "        <route id=\"r_9\" edges=\"2i 1o\"/>\"\"\", file=routes)\n",
    "\n",
    "        vehicle_num = 0\n",
    "        vclasses = [\"car\",\"truck\"]\n",
    "        routes_dict = {'WE':['r_6','r_7','r_8'],'SN':['r_0','r_1','r_2'],'EW':['r_3','r_4','r_5'],'NS':['r_9','r_10','r_11']}\n",
    "\n",
    "        weights_vclass = [10,1]\n",
    "        weights_route = [1,1,1]\n",
    "\n",
    "        for i in range(N):\n",
    "\n",
    "            if random.uniform(0, 1) < pWE:\n",
    "                vclass_type = random.choices(vclasses,weights=weights_vclass)[0]\n",
    "                v_route = random.choices(routes_dict['WE'],weights = weights_route)[0]\n",
    "                print(f'    <vehicle id=\"WE_{i}\" type=\"{vclass_type}\" route=\"{v_route}\" depart=\"{str(i)}\" />',file = routes)\n",
    "                vehicle_num += 1\n",
    "            \n",
    "            if random.uniform(0, 1) < pEW:\n",
    "                vclass_type = random.choices(vclasses,weights=weights_vclass)[0]\n",
    "                v_route = random.choices(routes_dict['EW'],weights=weights_route)[0]\n",
    "                print(f'    <vehicle id=\"EW_{i}\" type=\"{vclass_type}\" route=\"{v_route}\" depart=\"{str(i)}\" color= \"1,0,0\"/>',file = routes)\n",
    "                vehicle_num += 1\n",
    "            \n",
    "            if random.uniform(0, 1) < pNS:\n",
    "                vclass_type = random.choices(vclasses,weights=weights_vclass)[0]\n",
    "                v_route = random.choices(routes_dict['NS'],weights=weights_route)[0]\n",
    "                print(f'    <vehicle id=\"NS_{i}\" type=\"{vclass_type}\" route=\"{v_route}\" depart=\"{str(i)}\" color=\"0,1,0\"/>',file = routes)\n",
    "                vehicle_num += 1\n",
    "            \n",
    "            if random.uniform(0, 1) < pSN:\n",
    "                vclass_type = random.choices(vclasses,weights=weights_vclass)[0]\n",
    "                v_route = random.choices(routes_dict['SN'],weights=weights_route)[0]\n",
    "                print(f'    <vehicle id=\"SN_{i}\" type=\"{vclass_type}\" route=\"{v_route}\" depart=\"{str(i)}\" color=\"0,0,1\"/>',file = routes)\n",
    "                vehicle_num += 1\n",
    "        print(\"</routes>\", file=routes)\n",
    "        logger.info(\"total no of vechiles generated: {}\".format(vehicle_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)  # Red and green phases only\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(25,), dtype=np.float32)\n",
    "\n",
    "        self.empty_flag = False\n",
    "        self.break_flag = False\n",
    "        \n",
    "    def reset(self,**kwargs):\n",
    "        try:\n",
    "            # logger.info(\"reset function entered\")           \n",
    "            return self._get_observation()\n",
    "        except Exception as e:\n",
    "            logger.error(\"An error occurred while resetting the environment: %s\", str(e))\n",
    "            raise e\n",
    "\n",
    "    def step(self, action):\n",
    "        try:\n",
    "            # logger.info(\"step function entered\")\n",
    "            for idx, tl_id in enumerate(traci.trafficlight.getIDList()):\n",
    "                traci.trafficlight.setPhase(tl_id, action)\n",
    "            traci.simulationStep()\n",
    "            next_observation = self._get_observation()\n",
    "            reward = self._calculate_reward()\n",
    "            done = self._is_done()\n",
    "            info = {}\n",
    "            # logger.info(\"Step %d completed.\", self.step_counter)\n",
    "            return next_observation, reward, done, info\n",
    "        except Exception as e:\n",
    "            logger.error(\"An error occurred during the step: %s\", str(e))\n",
    "            raise e\n",
    "\n",
    "    def _get_observation(self):\n",
    "        try:  # Begin try block\n",
    "\n",
    "            # logger.info(\"observation entered\")\n",
    "            waiting_times = []\n",
    "            densities = []\n",
    "            phases = []\n",
    "\n",
    "            for tl_id in traci.trafficlight.getIDList():\n",
    "                phases.append(traci.trafficlight.getPhase(tl_id))\n",
    "                lanes = traci.trafficlight.getControlledLanes(tl_id)\n",
    "                for lane in lanes:\n",
    "                    waiting_times.append(traci.lane.getLastStepHaltingNumber(lane))\n",
    "                    densities.append(traci.lane.getLastStepOccupancy(lane))\n",
    "\n",
    "            observation = np.concatenate((waiting_times, densities,phases)) \n",
    "            return observation\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"An error occurred while getting observation: %s\", str(e))\n",
    "            raise e  \n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        try:\n",
    "            # Initialize reward components\n",
    "            traffic_flow_reward = 0\n",
    "            traffic_delay_penalty = 0\n",
    "            queue_length_penalty = 0\n",
    "            safety_reward = 0\n",
    "\n",
    "            # Iterate over traffic lights\n",
    "            for tl_id in traci.trafficlight.getIDList():\n",
    "                lanes = traci.trafficlight.getControlledLanes(tl_id)\n",
    "                for lane in lanes:\n",
    "                    # Traffic flow reward: based on average speed of vehicles\n",
    "                    traffic_flow_reward += traci.lane.getLastStepMeanSpeed(lane)\n",
    "\n",
    "                    # traffic_flow_reward = int(traffic_flow_reward)\n",
    "\n",
    "                    # Traffic delay penalty: based on waiting time of vehicles\n",
    "                    traffic_delay_penalty += traci.lane.getLastStepHaltingNumber(lane)\n",
    "\n",
    "                    # Queue length penalty: based on length of vehicle queues\n",
    "                    queue_length_penalty += traci.lane.getLastStepVehicleNumber(lane)\n",
    "\n",
    "                    # Safety reward: based on number of collisions\n",
    "                    safety_reward += traci.simulation.getCollidingVehiclesNumber()\n",
    "\n",
    "        \n",
    "            # logger.info(\"rewards:{},{},{},{}\".format(traffic_flow_reward,traffic_delay_penalty,queue_length_penalty,safety_reward))        \n",
    "\n",
    "            # Combine individual rewards with appropriate weights\n",
    "            total_reward = (\n",
    "                traffic_flow_reward\n",
    "                - traffic_delay_penalty \n",
    "                - queue_length_penalty \n",
    "                - safety_reward\n",
    "            )\n",
    "            return total_reward\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"An error occurred while calculating reward: %s\", str(e))\n",
    "            raise e\n",
    "    \n",
    "    def _is_done(self):\n",
    "        traci_step = int(traci.simulation.getTime())\n",
    "        # logger.info(\"at step:{}\".format(traci_step))\n",
    "        total_vehicles_present = 0\n",
    "\n",
    "        maxstep_flag = False\n",
    "        empty_flag = False\n",
    "\n",
    "        if traci_step >= 10000:\n",
    "            maxstep_flag = True\n",
    "\n",
    "        checkpoint = traci_step >=500\n",
    "\n",
    "        if checkpoint:\n",
    "            # Check if any lane has vehicles\n",
    "            for tl_id in traci.trafficlight.getIDList():\n",
    "                lanes = traci.trafficlight.getControlledLanes(tl_id)\n",
    "                for lane in lanes:\n",
    "                    total_vehicles_present += traci.lane.getLastStepVehicleNumber(lane)\n",
    "\n",
    "            if total_vehicles_present == 0:\n",
    "                empty_flag = True\n",
    "        done_flag = maxstep_flag or empty_flag\n",
    "\n",
    "        return done_flag\n",
    "\n",
    "            \n",
    "    def close(self):\n",
    "        try:\n",
    "            traci.close()\n",
    "            logger.info(\"SUMO simulation closed.\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"An error occurred while closing SUMO simulation: %s\", str(e))\n",
    "            raise e\n",
    "        \n",
    "# Running reward statistics\n",
    "running_reward_mean = 0\n",
    "running_reward_std = 1e-6 \n",
    "\n",
    "def normalize_reward(reward):\n",
    "    global running_reward_mean, running_reward_std\n",
    "    running_reward_mean = 0.95 * running_reward_mean + 0.05 * reward\n",
    "    running_reward_std = np.sqrt(0.95 * running_reward_std**2 + 0.05 * (reward - running_reward_mean)**2)\n",
    "    normalized_reward = (reward - running_reward_mean) / (running_reward_std + 1e-5)\n",
    "    return normalized_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m total_rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_episodes\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 16\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m started.\u001b[39m\u001b[38;5;124m\"\u001b[39m, episode)\n\u001b[0;32m     18\u001b[0m     episode_reward \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize environment\n",
    "env = CustomEnv()\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Train PPO agent\n",
    "# model = A2C('MlpPolicy', env, verbose=1,learning_rate=0.001,ent_coef=0.01)\n",
    "# model = PPO('CnnPolicy', env, verbose=1,)\n",
    "model = DQN('MlpPolicy', env, verbose=0, learning_rate=0.001, exploration_final_eps=0.001, tensorboard_log = None)\n",
    "# Set number of episodes\n",
    "num_episodes = 1000\n",
    "total_rewards = []\n",
    "\n",
    "\n",
    "for episode in range(1,num_episodes+1):\n",
    "    # logger.info(\"\\n\")\n",
    "    logger.info(\"\\n Episode %d started.\", episode)\n",
    "    episode_reward =0\n",
    "    generate_routefile()  # Generate route file \n",
    "    logger.info(\"Route file generated\")\n",
    "\n",
    "    tripinfo_name = \"tripinfo_epi{}.xml\".format(episode)\n",
    "    traci.start([\"sumo\", \"-c\", configfile_path,\"--no-warnings\", \"--tripinfo-output\", tripinfo_name])\n",
    "    logger.info(\"SUMO simulation started.\")\n",
    "\n",
    "    # Resetting environment at the beginning of each episode\n",
    "\n",
    "    obs= env.reset()\n",
    "    logger.info(\"Environmnet reset done\")\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        # Take action in the environment\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        # Update observation for next step\n",
    "        obs = next_obs\n",
    "        reward = normalize_reward(reward[0])\n",
    "        # print(reward)\n",
    "        episode_reward += reward\n",
    "\n",
    "    traci_steps = int(traci.simulation.getTime())\n",
    "\n",
    "    logger.info(\"at model.learn step\")\n",
    "    # Train agent\n",
    "    model.learn(total_timesteps=1)\n",
    " \n",
    "    total_rewards.append(episode_reward)\n",
    "\n",
    "    print(\"Total reward for episode {}: {}, completed in steps: {}\".format(episode, episode_reward,traci_steps))\n",
    "    logger.info(\"Total reward for episode {}: {}, completed in steps: {}\".format(episode, episode_reward,traci_steps))\n",
    "    env.close()\n",
    "    logger.info(\"traci is closed for episode\")\n",
    "\n",
    "\n",
    "    if episode%10 == 0:\n",
    "        model.save(\"model_{}\".format(episode))\n",
    "        logger.info(\"Model saved successfully.\")\n",
    "\n",
    "plt.plot(total_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('Reward per Episode')\n",
    "plt.savefig('total_rewards_plots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, env, num_episodes=5):\n",
    "    \n",
    "    generate_routefile()\n",
    "    total_eval_rewards = []\n",
    "    for episode in range(1, num_episodes+1):\n",
    "        traci.start([\"sumo\", \"-c\", configfile_path,\"--no-warnings\"])\n",
    "        eval_episode_reward = 0\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            reward = normalize_reward(reward[0])\n",
    "            eval_episode_reward += reward\n",
    "        total_eval_rewards.append(eval_episode_reward)\n",
    "        traci.close()\n",
    "    avg_reward = sum(total_eval_rewards) / num_episodes\n",
    "    \n",
    "    return total_eval_rewards, avg_reward\n",
    "\n",
    "logger.info(\"Evaluation Started\")\n",
    "\n",
    "print(\"\\n--------------Evalutaion Started------------------\")\n",
    "# Assuming 'model' and 'env' are already created\n",
    "episode_rewards, avg_reward = evaluate_model(model, env, num_episodes=5)\n",
    "print(\"Average Reward over 5 episodes: {}\".format(avg_reward))\n",
    "\n",
    "# Plot rewards per episode\n",
    "plt.plot(episode_rewards, marker='o')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('Reward per Episode (Evaluation)')\n",
    "plt.grid(True)\n",
    "plt.savefig('total_rewards_plots_eval.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
